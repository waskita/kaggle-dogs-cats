{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# this code is necessary only if you have more than 1 GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import os\n",
    "from keras.layers import Flatten, Dense, AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ada contoh binary classification dengan flow_from_directory di sini:\n",
    "\n",
    "https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8\n",
    "\n",
    "https://jkjung-avt.github.io/keras-tutorial/\n",
    "\n",
    "https://medium.com/@parthvadhadiya424/hello-world-program-in-keras-with-cnn-dog-vs-cat-classification-efc6f0da3cc5\n",
    "\n",
    "https://www.kaggle.com/sentdex/full-classification-example-with-convnet\n",
    "\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First part is splitting dataset into training set and validation set. For this demo we only use 1000 images, 800 images as training set, and 200 images as validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train directory :  ../simple1000/train\n",
      "valid directory :  ../simple1000/valid\n",
      "number of classes: 2\n",
      "logfile   : simple1000-train.log\n"
     ]
    }
   ],
   "source": [
    "# preparing parameters     \n",
    "image_dir_cat='../data/train/cat' # assuming cat & dog images has been separated in  different directories\n",
    "image_dir_dog='../data/train/dog'\n",
    "\n",
    "\n",
    "session = \"simple1000\" # to differentiate between runs\n",
    "ClassNames = ['cat', 'dog']\n",
    "data_dir=\"../simple1000\" # to differentiate between runs\n",
    "learning_rate = 0.0001\n",
    "img_width = 331 # 331 for pre-trained nasnet\n",
    "img_height = 331\n",
    "nbr_epochs = 10 \n",
    "batch_size = 4 # batch size depends on available memory on GPU. GTX 1080 Ti use (4)\n",
    "np.random.seed(2018)\n",
    "train_dir = data_dir + \"/train\"\n",
    "valid_dir = data_dir + \"/valid\"    \n",
    "number_of_class=len(ClassNames)\n",
    "print(\"train directory : \", train_dir)\n",
    "print(\"valid directory : \", valid_dir)    \n",
    "print(\"number of classes: \"+ str(number_of_class))\n",
    "  \n",
    "logfile = session + '-train' + '.log'\n",
    "print(\"logfile   :\", logfile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy start\n",
      "copy cats\n",
      "copy dogs\n",
      "copy finished\n"
     ]
    }
   ],
   "source": [
    "# make training directory\n",
    "# make validation directory\n",
    "# copy images to respective directories\n",
    "print(\"copy start\")        \n",
    "def MakeDir(newdir):\n",
    "    if not os.path.exists(newdir):\n",
    "        os.makedirs(newdir)\n",
    "\n",
    "        # make validation & training directories, if not exist yet        \n",
    "MakeDir(valid_dir)\n",
    "MakeDir(valid_dir+'/cat')\n",
    "MakeDir(valid_dir+'/dog')\n",
    "MakeDir(train_dir)\n",
    "MakeDir(train_dir+'/cat')\n",
    "MakeDir(train_dir+'/dog')\n",
    "\n",
    "# copy files to working directories\n",
    "print(\"copy cats\")\n",
    "counter=0\n",
    "for root, dirs, files in os.walk(image_dir_cat):\n",
    "    for file in files:\n",
    "        fullfilename = os.path.join(root, file)\n",
    "#        print(str(counter) + \": \" + fullfilename)\n",
    "        if(counter<800):\n",
    "            shutil.copyfile(fullfilename,train_dir+\"/cat/\"+file)            \n",
    "        if(counter>=800 and counter<1000):\n",
    "            shutil.copyfile(fullfilename,valid_dir+\"/cat/\"+file)\n",
    "        if(counter>=1000):\n",
    "            break\n",
    "        counter=counter+1                       \n",
    "print(\"copy dogs\")            \n",
    "counter=0        \n",
    "for root, dirs, files in os.walk(image_dir_dog):\n",
    "    for file in files:\n",
    "        fullfilename = os.path.join(root, file)\n",
    "#        print(str(counter) + \": \" + fullfilename)\n",
    "        if(counter<800):\n",
    "            shutil.copyfile(fullfilename,train_dir+\"/dog/\"+file)            \n",
    "        if(counter>=800 and counter<1000):\n",
    "            shutil.copyfile(fullfilename,valid_dir+\"/dog/\"+file)\n",
    "        if(counter>=1000):\n",
    "            break\n",
    "        counter=counter+1\n",
    "print(\"copy finished\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model with transfer learning\n",
    "if(True):\n",
    "    model_notop = keras.applications.nasnet.NASNetLarge(input_shape=(img_width, img_height, 3),\n",
    "                                                                  include_top=False,\n",
    "                                                                  weights='imagenet', input_tensor=None,\n",
    "                                                                  pooling=None)\n",
    "        # add a global spatial average pooling layer\n",
    "    x = model_notop.output\n",
    "    x = GlobalAveragePooling2D()(x)        \n",
    "    x = Dense(1024, activation='relu')(x) # let's add a fully-connected layer        \n",
    "    x = BatchNormalization()(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    deep_model = Model(model_notop.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare train generator\n",
      "Found 1600 images belonging to 2 classes.\n",
      "prepare validation generator\n",
      "Found 400 images belonging to 2 classes.\n",
      "fit generator\n",
      "Epoch 1/10\n",
      "400/400 [==============================] - 271s 677ms/step - loss: 0.3706 - acc: 0.8287 - val_loss: 0.1626 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96250, saving model to simple1000-weights.01-0.16.h5\n",
      "Epoch 2/10\n",
      "400/400 [==============================] - 222s 556ms/step - loss: 0.3033 - acc: 0.8769 - val_loss: 0.1101 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96250 to 0.97000, saving model to simple1000-weights.02-0.11.h5\n",
      "Epoch 3/10\n",
      "400/400 [==============================] - 223s 557ms/step - loss: 0.2929 - acc: 0.8731 - val_loss: 0.1433 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.97000\n",
      "Epoch 4/10\n",
      "400/400 [==============================] - 223s 556ms/step - loss: 0.2903 - acc: 0.8931 - val_loss: 0.1143 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.97000 to 0.98000, saving model to simple1000-weights.04-0.11.h5\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "#  training\n",
    "if(True):\n",
    "    sgd_optimizer = SGD(lr=learning_rate, momentum=0.9, decay=0.0, nesterov=True)\n",
    "    deep_model.compile(loss='binary_crossentropy', optimizer=sgd_optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # set up callbacks\n",
    "    \n",
    "    csv_logger = CSVLogger(logfile, append=True)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=1, mode='auto')\n",
    "    best_model_file=session+'-weights.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "    # best_model_file = session + '-weights' + '.h5'\n",
    "    best_model = ModelCheckpoint(best_model_file, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "    # this is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "    val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    print('prepare train generator')\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,        \n",
    "        class_mode='binary')\n",
    "    print('prepare validation generator')\n",
    "    validation_generator = val_datagen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        class_mode='binary')\n",
    "    print('fit generator')\n",
    "    deep_model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        #        steps_per_epoch=nbr_train_samples/batch_size, # in Keras 2.2.0, automatically acquired from train generator\n",
    "        epochs=nbr_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=validation_generator,\n",
    "        #        validation_steps=nbr_validation_samples/batch_size, # automatically acquired from validation generator\n",
    "        callbacks=[best_model, csv_logger, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n",
      "Loading model and weights\n",
      "Begin to predict for testing data ...\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "nbr_test_samples=12500 \n",
    "\n",
    "#choose weights file manually\n",
    "weights_path = 'simple1000-weights.07-0.09.h5'\n",
    "test_data_dir = '../data/test/'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False, # no shuffling, since filenames must match predictions. Shuffling may change file sequence\n",
    "        classes = None, # \n",
    "        class_mode = None)\n",
    "\n",
    "test_image_list = test_generator.filenames\n",
    "print('Loading model and weights')\n",
    "predict_model = load_model(weights_path)\n",
    "\n",
    "print('Begin to predict for testing data ...')\n",
    "predictions = predict_model.predict_generator(test_generator, nbr_test_samples)\n",
    "\n",
    "np.savetxt(session+'-predictions.txt', predictions) # store prediction matrix, for later analysis if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to write submission file:simple1000-submit.csv\n",
      "Finished write submission file ..\n"
     ]
    }
   ],
   "source": [
    "# submission\n",
    "submission_file=session+'-submit.csv'\n",
    "print('Begin to write submission file:'+submission_file)\n",
    "\n",
    "f_submit = open(submission_file, 'w')\n",
    "f_submit.write('id,label\\n')\n",
    "for i, image_name in enumerate(test_image_list):\n",
    "    basename=os.path.basename(image_name)\n",
    "    filename, fileext = os.path.splitext(basename)    \n",
    "    prediction_class    =predictions[i][0] # get predictions from array        \n",
    "    f_submit.write('%s,%s\\n' % (filename, prediction_class))\n",
    "f_submit.close()\n",
    "print('Finished write submission file ..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36-tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-py36-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
